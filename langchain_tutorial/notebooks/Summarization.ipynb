{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "173ca196-34d9-43a5-9b86-f29f462e8c76",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dimitarpg13/langchain_tutorial/blob/main/langchain_tutorial/notebooks/Summarization.ipynb)\n",
    "\n",
    "\n",
    "### Use Case\n",
    "\n",
    "Suppose you have a set of documents (PDFs, Notion pages, customer questions, etc) and you want to summarize the content.\n",
    "A central question for building a summarizer is how to pass your documents into the LLM's context window. Two common approaches for this are:\n",
    "\n",
    "1. `Stuff`: simply _stuff_ all your documents into a single promnpt. This is the simplest approach.\n",
    "2. `Map-reduce`: Summarize each document on its own in a _map_ step and then _reduce_ the summaries into a final summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f26900b-5c71-42af-b803-82ade0da1400",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain_openai langchain_core langchain_community tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eceb7822-59e0-44b9-a6ba-f4b5ee434fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59cfbb9f-6da5-48b8-a974-7b792e3f7831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05050ced-111c-4607-a551-0f3340dd7ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU \"langchain[openai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62f6e3ed-c728-4c35-b84f-e2673b0e0a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e717d69-5f46-42f4-9eb1-56fc4d0c17c8",
   "metadata": {},
   "source": [
    "## Stuff: summarize in a single LLM call\n",
    "\n",
    "We can use [create_stuff_documents_chain](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html), especially if using models with larger context window (that is, at least hundred of kilobytes).\n",
    "\n",
    "Examples of models with large context windows:\n",
    "\n",
    "* 128K token OpenAI `gpt-4o`\n",
    "* 200K token Anthropic `claude-3-5-sonnet-20240620`\n",
    "\n",
    "The chain will take a list of documents, insert them all into a prompt , and pass that prompt to an LLM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9b29c97-0191-4655-b388-4b0439ec80d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article by Lilian Weng discusses the concept of Large Language Model (LLM) powered autonomous agents, outlining their architecture and functionality in a detailed manner. It covers three main components: \n",
      "\n",
      "1. **Planning:** These agents apply techniques for task decomposition, allowing them to break large tasks into manageable subgoals and engage in self-reflection to improve subsequent actions.\n",
      "\n",
      "2. **Memory:** They utilize short-term and long-term memory, leveraging external vector stores for efficient retrieval of information, with concepts like Maximum Inner Product Search (MIPS) to enhance learning and recall.\n",
      "\n",
      "3. **Tool Use:** Integrating external tool capabilities, the agents can call APIs for additional information and utilize various expert modules, thereby extending their operational capabilities.\n",
      "\n",
      "The article highlights challenges such as finite context lengths, difficulties in long-term planning, and the reliability of natural language interfaces. It concludes with practical examples, such as the ChemCrow agent for scientific discovery and generative agents that simulate human behavior in interactive settings.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")]\n",
    ")\n",
    "\n",
    "# Instantiate chain\n",
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Invoke chain\n",
    "result = chain.invoke({\"context\": docs})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cf0b29-0c21-4c51-8275-f885b39c39a8",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "Note that we can also stream the result token-by-token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24fdb384-b861-40fa-8c15-996b84797406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|The| article| \"|LL|M| Powered| Autonomous| Agents|\"| by| Lil|ian| W|eng| discusses| the| development| and| potential| applications| of| autonomous| agents| driven| by| large| language| models| (|LL|Ms|).| It| provides| an| overview| of| the| key| components| that| comprise| L|LM|-powered| systems|:| planning|,| memory|,| and| tool| use|.\n",
      "\n",
      "|1|.| **|Planning|**|:| Agents| can| de|compose| large| tasks| into| smaller| sub|go|als| and| reflect| on| past| actions| to| improve| future| outcomes| using| techniques| like| Chain| of| Thought| (|Co|T|)| and| Tree| of| Thoughts| (|To|T|).\n",
      "\n",
      "|2|.| **|Memory|**|:| The| discussion| includes| types| of| memory| analogous| to| human| memory| â€”| sensory| memory|,| short|-term| memory| (|in|-context| learning|),| and| long|-term| memory| (|external| vector| storage|).| Techniques| to| optimize| retrieval| include| maximum| inner| product| search| (|M|IPS|)| using| algorithms| like| L|SH|,| AN|NO|Y|,| and| FA|ISS|.\n",
      "\n",
      "|3|.| **|Tool| Use|**|:| The| ability| to| access| external| tools| expands| the| functionality| of| L|LM|s|.| Examples| include| MR|KL| systems|,| which| route| tasks| to| expert| modules|,| and| frameworks| like| Hug|ging|GPT| and| API|-B|ank|,| which| provide| structured| approaches| for| using| multiple| APIs| for| complex| tasks|.\n",
      "\n",
      "|The| article| concludes| by| discussing| challenges| in| creating| reliable| L|LM|-powered| agents|,| including| limitations| in| context| length|,| difficulties| in| long|-term| planning|,| and| the| reliability| of| natural| language| outputs|.| It| cites| several| proof|-of|-con|cept| examples| like| Auto|GPT|,| which| highlight| the| potential| and| obstacles| of| L|LM|s| in| achieving| true| autonomy| in| complex| environments|.||"
     ]
    }
   ],
   "source": [
    "for token in chain.stream({\"context\": docs}):\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a000b47-e1ea-453f-b314-003c168f9078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
