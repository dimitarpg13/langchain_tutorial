{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fb0ebaf1",
      "metadata": {
        "id": "fb0ebaf1"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimitarpg13/langchain_tutorial/blob/main/langchain_tutorial/notebooks/runtime_config/ConfigurableLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c52ea2f9-03ff-4647-b782-46867ebed04e",
      "metadata": {
        "id": "c52ea2f9-03ff-4647-b782-46867ebed04e"
      },
      "source": [
        "# Working with Runtime Configurations\n",
        "\n",
        "\n",
        "We want to configure our graph when calling it. For example, we want to specify LLM or system prompt to use at runtime _without polluting the graph state with these parameters_.\n",
        "\n",
        "Adding runtime configuration, invloves a) specifying a schema for the configuration, b) adding the configuration to the nodes or conditional edges and c) passing the configuration to the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5197aba-5d46-421b-ae3b-4e3034edcfda",
      "metadata": {
        "id": "d5197aba-5d46-421b-ae3b-4e3034edcfda"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langchain_core langgraph langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speciyfing LLM at runtime\n",
        "\n",
        "Below is a simple example which illustrates how to specify LLM at runtime using the Runtime Configuration functionality:"
      ],
      "metadata": {
        "id": "Y9X55HdmQAnk"
      },
      "id": "Y9X55HdmQAnk"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableConfig\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langgrap.graph import MessageState"
      ],
      "metadata": {
        "id": "TYgbJnpVQEcG"
      },
      "id": "TYgbJnpVQEcG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Specify the config schema"
      ],
      "metadata": {
        "id": "vR3DKEV0S6x9"
      },
      "id": "vR3DKEV0S6x9"
    },
    {
      "cell_type": "code",
      "source": [
        "class ConfigSchema(TypedDict):\n",
        "    model: str\n",
        "\n"
      ],
      "metadata": {
        "id": "wBxBSHvSTRVS"
      },
      "id": "wBxBSHvSTRVS",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}