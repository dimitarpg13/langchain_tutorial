{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvkJSLg+R57Loi9lnDQyIV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimitarpg13/langchain_tutorial/blob/main/langchain_tutorial/notebooks/runtime_config/ConfigurableLLM_ver2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c52ea2f9-03ff-4647-b782-46867ebed04e"
      },
      "source": [
        "# Working with Runtime Configurations\n",
        "\n",
        "\n",
        "We want to configure our graph when calling it. For example, we want to specify LLM or system prompt to use at runtime _without polluting the graph state with these parameters_.\n",
        "\n",
        "Adding runtime configuration, invloves a) specifying a schema for the configuration, b) adding the configuration to the nodes or conditional edges and c) passing the configuration to the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5197aba-5d46-421b-ae3b-4e3034edcfda"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph langchain langchain_core  langchain_openai langchain-anthropic dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lcE41jtjXUNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specify the necessary keys"
      ],
      "metadata": {
        "id": "sIARhfaWRqEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")\n",
        "\n",
        "_set_env(\"ANTHROPIC_API_KEY\")\n"
      ],
      "metadata": {
        "id": "FGY-i9hoRvPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speciyfing LLM at runtime\n",
        "\n",
        "Below is a simple example which illustrates how to specify LLM at runtime using the Runtime Configuration functionality:"
      ],
      "metadata": {
        "id": "Y9X55HdmQAnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langgraph.graph import END, StateGraph, START, MessagesState\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n"
      ],
      "metadata": {
        "id": "TYgbJnpVQEcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Specify the context schema"
      ],
      "metadata": {
        "id": "vR3DKEV0S6x9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODELS = {\n",
        "    \"anthropic\": \"anthropic:claude-3-5-haiku-latest\",\n",
        "    \"openai\": \"openai:gpt-4.1-mini\"\n",
        "}\n",
        "\n",
        "class ConfigSchema(TypedDict):\n",
        "    model_provider: str\n",
        "\n",
        "def model(state: MessagesState, config: RunnableConfig):\n",
        "    if \"model_provider\" in config[\"configurable\"]:\n",
        "      if config[\"configurable\"][\"model_provider\"] == \"anthropic\":\n",
        "          chat_model = init_chat_model(MODELS[\"anthropic\"])\n",
        "          response = chat_model.invoke(state[\"messages\"])\n",
        "          return {\"messages\": [response]}\n",
        "      elif config[\"configurable\"][\"model_provider\"] == \"openai\":\n",
        "          chat_model = init_chat_model(MODELS[\"openai\"])\n",
        "          response = chat_model.invoke(state[\"messages\"])\n",
        "          return {\"messages\": [response]}\n",
        "      else:\n",
        "          raise ValueError(\"Unknown values.\")\n",
        "    else:\n",
        "      # config/model_provider not specified, detault to anthropic\n",
        "      chat_model = init_chat_model(MODELS[\"anthropic\"])\n",
        "      response = chat_model.invoke(state[\"messages\"])\n",
        "      return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wBxBSHvSTRVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kOW49AuEIs9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "builder = StateGraph(MessagesState, config_schema=ConfigSchema)\n",
        "builder.add_node(model)\n",
        "builder.add_edge(START, \"model\")\n",
        "builder.add_edge(\"model\", END)\n",
        "\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "id": "ucfDZtftC81y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "User the graph with test input message"
      ],
      "metadata": {
        "id": "qnQ-AwQl3j53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_message = {\"role\": \"user\", \"content\": \"hi\"}"
      ],
      "metadata": {
        "id": "Qmjo-Lzg3nbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With no configuration, this graph uses default (Anthropic)"
      ],
      "metadata": {
        "id": "ZVOmetNy5VXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_1 = graph.invoke({\"messages\": [input_message]})[\"messages\"][-1]\n",
        "print(response_1.response_metadata[\"model_name\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H5P15NR5dJH",
        "outputId": "31f5f348-f86c-405d-dd8c-dbd368135db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "claude-3-5-haiku-20241022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "with configuration supplied it opens what is specified:"
      ],
      "metadata": {
        "id": "e_e4_oGcFHzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_2 = graph.invoke({\"messages\": [input_message]}, config=ConfigSchema(model_provider=\"openai\"))[\"messages\"][-1]\n",
        "response_3 = graph.invoke({\"messages\": [input_message]}, config=ConfigSchema(model_provider=\"anthropic\"))[\"messages\"][-1]\n",
        "\n",
        "print(response_2.response_metadata[\"model_name\"])\n",
        "print(response_3.response_metadata[\"model_name\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_dsQWfoH9-N",
        "outputId": "c5dfe16f-c260-4cb7-f413-c0365e8a7b94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gpt-4.1-mini-2025-04-14\n",
            "claude-3-5-haiku-20241022\n"
          ]
        }
      ]
    }
  ]
}