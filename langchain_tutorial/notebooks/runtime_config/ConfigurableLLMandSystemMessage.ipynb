{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfSZAeklyQQmnddUgFmk6l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimitarpg13/langchain_tutorial/blob/main/langchain_tutorial/notebooks/runtime_config/ConfigurableLLMandSystemMessage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working with Runtime Configurations\n",
        "\n",
        "We want to configure our graph when calling it. For example, we want to specify LLM or system prompt to use at runtime without polluting the graph state with these parameters.\n",
        "\n",
        "Adding runtime configuration, invloves a) specifying a schema for the configuration, b) adding the configuration to the nodes or conditional edges and c) passing the configuration to the graph"
      ],
      "metadata": {
        "id": "AiZLhmUJiQTL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5TNCiTuuiG8a"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph==0.6.0a2 langchain langchain_core  langchain_openai langchain-anthropic dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specify the necessary keys"
      ],
      "metadata": {
        "id": "bXZ9d4AniYm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")\n",
        "\n",
        "_set_env(\"ANTHROPIC_API_KEY\")"
      ],
      "metadata": {
        "id": "ZSIzpzxhibMR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speciyfing LLM at runtime\n",
        "\n",
        "Below is a simple example in which we configure two parameters at runtime using the Runtime Configuration functionality: the LLM and system message"
      ],
      "metadata": {
        "id": "wL4-mMJWieSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langgraph.graph import END, StateGraph, START, MessagesState\n",
        "from typing_extensions import TypedDict\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langgraph.runtime import Runtime"
      ],
      "metadata": {
        "id": "hIux588Yids_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specify the context schema"
      ],
      "metadata": {
        "id": "B2pD4Ii7gZIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ContextSchema:\n",
        "    model_provider: str = \"anthropic\"\n",
        "    system_message: str | None = None\n",
        "\n",
        "MODELS = {\n",
        "    \"anthropic\": init_chat_model(\"anthropic:claude-3-5-haiku-latest\"),\n",
        "    \"openai\": init_chat_model(\"openai:gpt-4.1-mini\")\n",
        "}"
      ],
      "metadata": {
        "id": "jzYW9bD9gbbM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "define call_model and construct the graph"
      ],
      "metadata": {
        "id": "0rbJb66LgfcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def call_model(state: MessagesState, runtime: Runtime[ContextSchema]):\n",
        "    model = MODELS[runtime.context['model_provider']]\n",
        "    messages = state[\"messages\"]\n",
        "    if (system_message := runtime.context['system_message']):\n",
        "        messages = [SystemMessage(system_message)] + messages\n",
        "    response = model.invoke(messages)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "builder = StateGraph(MessagesState, context_schema=ContextSchema)\n",
        "builder.add_node(\"model\", call_model)\n",
        "builder.add_edge(START, \"model\")\n",
        "builder.add_edge(\"model\", END)\n",
        "\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "id": "lM9SV1paiD_4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "User the graph with test input message"
      ],
      "metadata": {
        "id": "mOUdh7WwsvyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_message = {\"role\": \"user\", \"content\": \"hi\"}\n",
        "response = graph.invoke({\"messages\": [input_message]}, context={\"model_provider\": \"openai\", \"system_message\": \"Respond in Italian.\"})\n",
        "for message in response[\"messages\"]:\n",
        "    message.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BNCQrdeswLk",
        "outputId": "f92175ea-fb76-4a1b-ca6d-f66d0dc0ed52"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "hi\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Ciao! Come posso aiutarti oggi?\n"
          ]
        }
      ]
    }
  ]
}